# **Longhand** #
Longhand is a python notebook and associated Blender python (bpy) scripts that, combined, takes a text document and returns an immersive visualization (.glb file) of common words/tokens/entities for use in [Mozilla Hubs](https://hubs.mozilla.com/). Longhand allows non-technical end users to explore unwieldy text corpora (including in VR) early in the research lifecycle. Longhand is an extension of Mike Hucka's [HandPrint](https://github.com/caltechlibrary/handprint) tool and subsequent data management efforts by Harvard Library, including the [SAEF_OCR](https://github.com/Cook4986/SAEF_OCR) project.

![throughput diagram](https://images.squarespace-cdn.com/content/v1/532b70b6e4b0dca092974dbe/1627401430752-R7H10DTUUOSB4GKDDKD1/Longhand+Throughput_Cook2021.png?format=2500w)

Longhand takes as input plain text of the sort generated by Mike Hucka's [Handprint](https://github.com/caltechlibrary/handprint) handwriting transcription (HTR) tool. Those individual transcriptions represent the textual contents of digital images, which are more common in many archives than full text transcriptions. Fortunately, big-tech OCR APIs - like Microsoft's [Read API](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/vision-api-how-to-topics/call-read-api) - can accurately transcribe such images , although manual analysis of the resulting plain text is still required. That's where Longhand comes in. 

Beginning with the notebook script ("Longhand_notebook"), Longhand inputs a plain text ["bag of words"-type](https://en.wikipedia.org/wiki/Bag-of-words_model) document. That document is subjected to natural language processing, via [SpaCy](https://spacy.io/), whereby a researcher can customize the script to specify target parts-of-speech (e.g. nouns) or [named entity types](https://github.com/mchesterkadwell/named-entity-recognition), like people, objects, or locations. 

Once the bag-of-words has been parsed by chosen sub-type, the "x" (number of) most common words are added to a dictionary of lists, in order, based their relative frequency in the transcribed corpus. That frequency - represented as a percentage - is included in a second output document ("...objects.txt" output)along with urls, specific 3d model names, and UIDs, all generated via the [Sketchfab API](https://sketchfab.com/developers/data-api/v3)

### Usage
Customize "declarations" and "input/output" lines in the [Longhand_notebook.ipynb](https://github.com/Cook4986/Longhand/blob/main/Longhand_notebook.ipynb) to generate "objects" documents in the target working directory. The notebook concludes with a terminal command that will launch Blender and run the first of three Blender Python ("bpy") scripts, [Longhand_downloader.py](https://github.com/Cook4986/Longhand/blob/main/Longhand_downloader.py). Once the Blender GUI launches, and the scene is populated with 3D models from the objects.txt doc, use [the Blender text editor](https://docs.blender.org/manual/en/2.79/editors/text_editor.html) run the [Longhand_aligner.py](https://github.com/Cook4986/Longhand/blob/main/Longhand_aligner.py) and [Longhand_exporter.py](https://github.com/Cook4986/Longhand/blob/main/Longhand_exporter.py), in sequence, to generate a binarized [.GLB](https://en.wikipedia.org/wiki/GlTF) file, which includes model textures and can be [uploaded to Mozilla Spoke](https://hubs.mozilla.com/docs/spoke-creating-projects.html) as an interactive, multi-user scene with XR support. 

### Innovations
* Supports “raw” text input data
* Leverages large, existing 3D asset collections
* Exposes text-centric fields to the benefits of XR
  * Depth cues
  * Embodied interactivity
  * Sense of scale
### Limitations
* Best for nouns (i.e. things) in the corpus
* Proprietary HTR (Amazon, Google, Microsoft)
* Model placement in scene is work-in-progress 
### Next Steps
* Link with [SAEF repo](https://github.com/Cook4986/SAEF_OCR)
* Upgrade model distribution function in "Longhand_aligner.py" script
* Automate GLB file hosting (AWS?) for easier Hubs integration
### Core Technologies
 * [HandPrint](https://github.com/caltechlibrary/handprint)(version: 1.5.1)
 * [SpaCy](https://github.com/explosion/spaCy)(3.2.1)
 * [Sketchfab data API, V3](https://docs.sketchfab.com/data-api/v3/index.html)
 * [Blender](https://www.blender.org/)(3.0.1)
 * [Blender 3.0.1 Python API](https://docs.blender.org/api/current/index.html)
 * [Mozilla Hubs](https://github.com/mozilla/hubs)
### Further Reading
* ["Perceiving layout and knowing distances: The integration, relative potency, and contextual use of different information about depth"](https://www.researchgate.net/profile/James-Cutting/publication/236964257_Perceiving_layout_and_knowing_distances_The_interaction_relative_potency_and_contextual_use_of_different_information_about_depth/links/0c96051a7a988e9232000000/Perceiving-layout-and-knowing-distances-The-interaction-relative-potency-and-contextual-use-of-different-information-about-depth.pdf) Cutting & Vishton 1995
* ["Promoting rotational-invariance in object recognition despite experience with only a single view"](https://www.sciencedirect.com/science/article/pii/S0376635715300735?casa_token=RFiw0OhRdPsAAAAA:7rb-Hsu-ZnPZs2l1iwr2g61yJCY4lXp6nfRIP299JcLv7G7L8EmALA3VzYyQ910dIfLKj1lh) Soto & Wasserman 2016
* ["Studying the effects of stereo, head tracking, and field of regard on a small-scale spatial judgment task"](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261311&casa_token=101RdCpGgAgAAAAA:tW7Hjpk6IvHNIcPI1gnoxbVBMCxtnU9sNHan2L0xB36jFL_Oz_kskc49IlVyb0YBsOcC5s0) Ragan et al. 2013
* ["Evaluating the benefits of the immersive space to think"](https://infovis.cs.vt.edu/sites/default/files/WEVR2020_Lisle.pdf) Lisle et al. 2020

Matt Cook [mncook.net](https://www.mncook.net/)- 2022
