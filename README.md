# **Longhand** #
Longhand is a python notebook and associated Blender python (bpy) scripts that, combined, takes a set of plain text transcriptions (representing a text corpus) and returns an immersive visualization of common entities for use in [Mozilla Hubs](https://hubs.mozilla.com/). Longhand allows non-technical end users to explore unwieldy text corpora early in the research lifecycle. Longhand is an extension of Mike Hucka's [HandPrint] tool and subsequent data management efforts by Harvard Library, including the [SAEF_OCR](https://github.com/Cook4986/SAEF_OCR) project.

![throughput diagram](https://images.squarespace-cdn.com/content/v1/532b70b6e4b0dca092974dbe/1627401430752-R7H10DTUUOSB4GKDDKD1/Longhand+Throughput_Cook2021.png?format=2500w)

Longhand takes as input plain text transcription files of the sort generated by Mike Hucka's [Handprint](https://github.com/caltechlibrary/handprint) tool. Those individual transcriptions represent the textual (including handwritten) contents of digital images. Emerging big-tech OCR APIs to, like Microsoft's [Read API](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/vision-api-how-to-topics/call-read-api), can transcribe such images with a high degree of accuracy, although manual analysis of the resulting plain text is still required. 

Beginning with the notebook script ("Longhand_notebook"), Longhand combines individual image transcriptions into a single plain text ["bag of words"-type](https://en.wikipedia.org/wiki/Bag-of-words_model) document. That document is then subjected to natural language processing, via [SpaCy](https://spacy.io/) where a researcher can customize the script to specify target parts-of-speech (e.g. nouns) or [named entity types](https://github.com/mchesterkadwell/named-entity-recognition), like people, objects, or locations. 

Once the bag-of-words has been parsed by sub-type, the x most common words are added to a dictionary of lists, in order, based their relative frequency in the transcribed corpus. That frequency - represented as a percentage - is included in a second output document along with urls, specific 3d model names, and UIDs, all generated via the [Sketchfab API](https://sketchfab.com/developers/data-api/v3)

### Usage
Customize "declarations" and "input/output" lines in the [Longhand_notebook.ipynb](https://github.com/Cook4986/Longhand/blob/main/Longhand_notebook.ipynb) to generate bag-of-words and "objects" documents in the target working directory. The notebook terminates with a terminal command that will launch Blender and run the first of three Blender Python ("bpy") scripts, [Longhand_downloader.py](https://github.com/Cook4986/Longhand/blob/main/Longhand_downloader.py). Once the Blender GUI launches, and the scene is populated with 3D models, run the [Longhand_aligner.py](https://github.com/Cook4986/Longhand/blob/main/Longhand_aligner.py) and [Longhand_exporter.py](https://github.com/Cook4986/Longhand/blob/main/Longhand_exporter.py), in sequence, to generate a binarized [.GLB](https://en.wikipedia.org/wiki/GlTF) file, which can be [uploaded to Mozilla Spoke](https://hubs.mozilla.com/docs/spoke-creating-projects.html) as an interactive, multi-user scene with XR support. 

### Innovations
* Supports “raw” input data, at scale (HTR)
* Leverages large 3D asset collections (Sketchfab)
* Exposes text-centric fields to the benefits of XR
  * Depth cues
  * Embodied interactivity
  * Sense of scale
### Limitations
* Best for nouns (i.e. things) in the corpus
* Proprietary HTR (Amazon, Google, Microsoft)
* Model placement in scene is unsolved 
### Next Steps
* Link with [SAEF repo](https://github.com/Cook4986/SAEF_OCR)
* Automate model distribution in Blender 
* Automate GLB file hosting for Hubs
### Core Technologies
 * [HandPrint](https://github.com/caltechlibrary/handprint)(version: 1.5.1)
 * [SpaCy](https://github.com/explosion/spaCy)(3.2.1)
 * [Sketchfab data API, V3](https://docs.sketchfab.com/data-api/v3/index.html)
 * [Blender](https://www.blender.org/)(3.0.1)
 * [Blender 3.0.1 Python API](https://docs.blender.org/api/current/index.html)
 * [Mozilla Hubs](https://github.com/mozilla/hubs)
### Further Reading
* ["Perceiving layout and knowing distances: The integration, relative potency, and contextual use of different information about depth"](https://www.researchgate.net/profile/James-Cutting/publication/236964257_Perceiving_layout_and_knowing_distances_The_interaction_relative_potency_and_contextual_use_of_different_information_about_depth/links/0c96051a7a988e9232000000/Perceiving-layout-and-knowing-distances-The-interaction-relative-potency-and-contextual-use-of-different-information-about-depth.pdf) Cutting & Vishton 1995
* ["Promoting rotational-invariance in object recognition despite experience with only a single view"](https://www.sciencedirect.com/science/article/pii/S0376635715300735?casa_token=RFiw0OhRdPsAAAAA:7rb-Hsu-ZnPZs2l1iwr2g61yJCY4lXp6nfRIP299JcLv7G7L8EmALA3VzYyQ910dIfLKj1lh) Soto & Wasserman 2016

Matt Cook [mncook.net](https://www.mncook.net/)- 2022
