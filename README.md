# **Longhand** #
Longhand takes images of handwritten text and returns an immersive visualization. 

![throughput diagram](https://images.squarespace-cdn.com/content/v1/532b70b6e4b0dca092974dbe/1627401430752-R7H10DTUUOSB4GKDDKD1/Longhand+Throughput_Cook2021.png?format=2500w)

### Innovations
* Supports “raw” input data, at scale (HTR)
* Leverages large 3D asset collections (Sketchfab)
* Exposes humanities to the benefits of XR 
### Limitations
* Best for nouns (i.e. things) in the corpus
* Proprietary HTR (Amazon, Google, Microsoft)
* Model placement in scene is unsolved 
### Next Steps
* Short term: Hack Hubs, Topic Modeling 
* Long term: Audio, Video, Image support
* Other collections 

## Core Technologies
 * [OpenCV](https://github.com/opencv/opencv)
 * [HandPrint](https://github.com/caltechlibrary/handprint)
 * [SpaCy](https://github.com/explosion/spaCy)
 * [Sketchfab data API, V3](https://docs.sketchfab.com/data-api/v3/index.html)
 * [Blender 3.0.1 Python API](https://docs.blender.org/api/current/index.html)
 * [Mozilla Hubs](https://github.com/mozilla/hubs)

Matt Cook - 2021
