{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Longhand takes a handwritten document corpus and returns an immersive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing for image OCR\n",
    "#Portions from Gabe Pizzorno 'Workflow-clean' script\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "#declarations\n",
    "path='xxx' #target image directory\n",
    "out='xxx' #transcription output path\n",
    "listFiles=os.listdir(path)\n",
    "imgNo = 0\n",
    "\n",
    "#file/os operations\n",
    "for file in sorted(listFiles):\n",
    "    if not file.startswith('.'):\n",
    "        print(path +'/'+ file)\n",
    "        image = cv2.imread(path +'/'+ file)\n",
    "        fileName=file.split('.')[0]\n",
    "        img_gs = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        img_bw = cv2.adaptiveThreshold(img_gs, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 91, 5)\n",
    "        img_inv = cv2.bitwise_not(img_bw)\n",
    "        cv2.imshow(fileName, img_inv)\n",
    "        cv2.imwrite(out+\"/\"+fileName+\".jpg\", img_inv) \n",
    "        print(fileName + \" pre-processed\")\n",
    "        cv2.waitKey(10)\n",
    "        imgNo = int(imgNo) + 1\n",
    "    cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Handprint from local directory\n",
    "!handprint --service microsoft,google,amazon-textract --debug - \"xxx\" --no-grid --extended --output \"xxx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Handprint using URLs from local .txt file\n",
    "!handprint --service microsoft,google,amazon-textract --from-file \"xxx\" --no-grid --extended --output \"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Bag-of-Words txt output and string search from HandPrint transcriptions. MNC - 5/21\n",
    "###Built to work with HandPrint, by Mike Hucka: https://github.com/caltechlibrary/handprint\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#paths\n",
    "cardPath = 'xxx'#plain text list of urls or filenames (pre-existing)\n",
    "outPath = 'xxx' #HP output (pre-existing)\n",
    "textOut = 'xxx'#Bag-of-Words output\n",
    "searchOut = 'xxx'#user search output\n",
    "listFiles = os.listdir(outPath)\n",
    "count = 0\n",
    "\n",
    "#declarations\n",
    "headers = [] \n",
    "transcriptions = []\n",
    "BoW = open(textOut, \"a\")\n",
    "\n",
    "#user search\n",
    "search = open(searchOut, \"a\")\n",
    "inputString = input(\"Search document for: \")\n",
    "inputString = str(inputString)\n",
    "\n",
    "#generate header list\n",
    "with open (cardPath, 'rt') as myfile: \n",
    "    for line in myfile:\n",
    "        headers.append(line)\n",
    "\n",
    "#append headers and transcriptions to bag-of-words \n",
    "for file in sorted (listFiles):\n",
    "    if not file.startswith('.') and file.endswith (\".handprint-microsoft.txt\"):\n",
    "    #if not file.startswith('.') and file.endswith (\".handprint-amazon-textract.txt\"):\n",
    "    #if not file.startswith('.') and file.endswith (\".handprint-google.txt\"):\n",
    "        BoW.write(\"\\n\")\n",
    "        BoW.write(headers[count]) #write card location to disk\n",
    "        #print(\"\\n\")\n",
    "        print((headers[count]) + \"added\")\n",
    "        #contents = open(outPath + \"/document-\" + str(count + 1) + \".handprint-microsoft.txt\", \"r\") #read card transcription\n",
    "        contents = open(outPath + \"/\" + file, \"r\") #read card transcription\n",
    "        transcriptions.append(contents.read())\n",
    "        #print(transcriptions[count])\n",
    "        #print(\"\\n\")\n",
    "        BoW.write(transcriptions[count]) #write transcriptions to disk\n",
    "        BoW.write(\"\\n\")\n",
    "        count = count + 1\n",
    "        #print(\"\\n\")    \n",
    "BoW.close()\n",
    "\n",
    "#search\n",
    "BoW = open(textOut, \"r\")\n",
    "count = 0\n",
    "\n",
    "while count < len(headers):\n",
    "    if inputString in transcriptions[count]:\n",
    "        print(headers[count])\n",
    "        print(transcriptions[count])\n",
    "        search.write(\"\\n\")\n",
    "        search.write(headers[count])\n",
    "        search.write(transcriptions[count])\n",
    "        search.write(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        count = count + 1\n",
    "    elif inputString not in transcriptions[count]:\n",
    "        #print(\"\\n\")\n",
    "        print(\"No such string found in transcription \" + str(count + 1))\n",
    "        #print(\"\\n\")\n",
    "        count = count + 1\n",
    "search.close()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"have a nice day\")\n",
    "        \n",
    "###To DO\n",
    "### more inclusive search\n",
    "### LocateXT integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###pdf parser\n",
    "###PyMuPDF: https://pymupdf.readthedocs.io/en/latest/\n",
    "\n",
    "import sys \n",
    "import fitz\n",
    "fname = \"xxx\"  # get document filename\n",
    "doc = fitz.open(fname)  # open document\n",
    "out = open(fname + \".txt\", \"wb\")  # open text output\n",
    "for page in doc:  # iterate the document pages\n",
    "    text = page.get_text().encode(\"utf8\")  # get plain text (is in UTF-8)\n",
    "    out.write(text)  # write text of page\n",
    "    out.write(bytes((12,)))  # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###Natural Language Processing\n",
    "##Modified from \"named-entity-recognition\" repo by Mary Chester-Kadwell\n",
    "##(https://github.com/mchesterkadwell/named-entity-recognition/blob/main/LICENSE)\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "#declarations\n",
    "nlp = en_core_web_sm.load()\n",
    "text_file = Path('data', 'xxx')\n",
    "body_parts = open(\"xxx\", \"r\")\n",
    "\n",
    "# Open the file, read it and store the text with the name `iliad`\n",
    "with open(text_file, encoding=\"utf-8\") as file:\n",
    "    iliad = file.read()\n",
    "\n",
    "#pass text to SpaCy language model()\n",
    "document = nlp(iliad)\n",
    "document.text\n",
    "\n",
    "#generate list of body common parts\n",
    "#Note:\"back\" removed\n",
    "list_of_parts = []\n",
    "for line in body_parts:\n",
    "    stripped_line = line.strip()\n",
    "    #line_list = stripped_line.split()\n",
    "    list_of_parts.append(stripped_line)\n",
    "#print(list_of_parts)\n",
    "\n",
    "#identify nouns in document\n",
    "nouns = []\n",
    "pos_tags = [(token.text, token.pos_, token.tag_) for token in document if token.text.isalpha()]\n",
    "for token in document:\n",
    "    if token.text in list_of_parts:\n",
    "        nouns.append(token.text)\n",
    "#print(nouns)\n",
    "\n",
    "persons = []\n",
    "for entity in document.ents:\n",
    "    if entity.label_ == \"PRODUCT\":\n",
    "        persons.append(entity.text)\n",
    "        #print(f'{entity.text}: {entity.label_}')\n",
    "        \n",
    "#print high-frenquency nouns\n",
    "word_freq = Counter(nouns)\n",
    "common_words = word_freq.most_common(25)\n",
    "print(common_words)\n",
    "\n",
    "# Display the plot inline in the notebook with interactive controls\n",
    "%matplotlib notebook\n",
    "\n",
    "# Get a list of the most common words\n",
    "words = [word for word,_ in common_words]\n",
    "\n",
    "# Get a list of the frequency counts for these words\n",
    "freqs = [count for _,count in common_words]\n",
    "\n",
    "# Set titles, labels, ticks and gridlines\n",
    "plt.title(\"Body part mentions in \"\"Women in Chains\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"# of appearances\")\n",
    "plt.xticks(range(len(words)), [str(s) for s in words], rotation=90)\n",
    "plt.grid(b=True, which='major', color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.gcf().subplots_adjust(bottom=0.35)\n",
    "\n",
    "# Plot the frequency counts\n",
    "plt.plot(freqs)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig('xxx', bbox_inches=\"tight\")\n",
    "\n",
    "#display named entities\n",
    "displacy.render(document, style=\"ent\")\n",
    "\n",
    "body_parts.close()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Topic Modeling (LDA) Longhand inputs\n",
    "###Cook 2021\n",
    "##portions of code from https://radimrehurek.com/gensim/auto_examples\n",
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import models\n",
    "from gensim.models import LdaModel\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#declarations\n",
    "doc = open(\"xxx\", \"r\")\n",
    "doc = doc.read()\n",
    "documents = [doc]\n",
    "\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in doc.lower().split() if word not in stoplist]\n",
    "    for doc in documents\n",
    "]\n",
    "#print(texts)\n",
    "\n",
    "#create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "\n",
    "#create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "\n",
    "doc.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
