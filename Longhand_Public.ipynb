{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6cf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Longhand - M. Cook - 2022\n",
    "##Takes Bag-of-Words and returns multi-user, immersive visualization\n",
    "##https://github.com/Cook4986/Longhand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcb409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes local image set and returns absolute filepaths (.txt) for Handprint input + lookup table (.csv)\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#inputs\n",
    "target =\"...\" #target image directory\n",
    "\n",
    "#outputs\n",
    "pathsOut = open(\"...\", \"w\") # for Handprint\n",
    "tableOut = \"...csv\"# transcription batch csv\n",
    "\n",
    "#dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#cross-check that DRS FILE-OSN values exist in target directory and add matches to pathsOut + dataframe\n",
    "for path in sorted(Path(target).rglob('*.JPG')):\n",
    "    absolute = (str(path.parent) + \"/\" + path.name) #absolute path for images\n",
    "    pathsOut.write(str(absolute)) #write paths to pathsOut\n",
    "    pathsOut.write(\"\\n\")\n",
    "    df = df.append({'NAME':path.stem,'IMG-PATH':absolute}, ignore_index=True) #append data frame  \n",
    "    print(\"image \" + path.stem + \" located \" + \"at \" + absolute) #console out\n",
    "\n",
    "#create new lookup table from dataframe\n",
    "with open(tableOut, mode = 'a') as f:\n",
    "    df.to_csv(f,index=False) #append tableOut with FILE-OSN and IMG-PATH values\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"lookup table created for corpus\")\n",
    "pathsOut.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ff1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Handprint using paths from local file\n",
    "##Mike Hucka designed and implemented Handprint beginning in mid-2018.\n",
    "##installation instructions at https://github.com/caltechlibrary/handprint\n",
    "\n",
    "##generate Microsoft results\n",
    "!handprint --service microsoft -@\"....txt\" --from-file \"....txt\" --no-grid --extended --output \"...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes preliminary lookup table (CSV) and outputs updated table with locations for Handprint outputs, by file type\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#Declarations\n",
    "table = \".../Baptismal Records/Lag 1838-1869/LookupTable.csv\" ##Lookup table\n",
    "hpOut =\".../Baptismal Records/Lag 1838-1869/Lag 1838-1869_MSFT\"##HandPrint Outputs\n",
    "types = ['MSFT-IMG','MSFT-JSON','MSFT-TXT']##additional lookup table columns\n",
    "suffX = ['.png','.json','.txt']#OCR file types\n",
    "num = 0 ##counter\n",
    "df = [1, 2, 3] #empty dataframes\n",
    "\n",
    "#match DRS \"FILE-OSN\" to OCR outputs and append new dataframes \n",
    "while num < len(df):  \n",
    "    df[num] = pd.DataFrame()\n",
    "    for path in sorted(Path(hpOut).rglob('*' + str(suffX[num]))): #Loop through file types\n",
    "        #define location and match to DRS's \"FILE-OSN\" column value\n",
    "        absolute = (str(path.parent) + \"/\" + path.name)\n",
    "        match = path.stem.split('.')\n",
    "        fileCore = pd.read_csv(table, usecols=['FILE-OSN'])\n",
    "        #append dataframe with matching values\n",
    "        for value in fileCore.values:\n",
    "            if value == match[0]:\n",
    "                df[num] = df[num].append({types[num]:absolute}, ignore_index=True)\n",
    "                print(\"\\n\")\n",
    "                print(\"dataframe \" + str(num + 1) + \" appended with \" + suffX[num] + \" located at \" + absolute)\n",
    "    num = num + 1   \n",
    "    \n",
    "#concatenate newly appended data frames with existing lookup table\n",
    "conC = pd.concat(df, axis=1)\n",
    "print(\"\\n\")\n",
    "print(\"new dataframes concatenated\")\n",
    "out = pd.concat([pd.read_csv(table),conC], axis=1)\n",
    "\n",
    "#update lookup table CSV\n",
    "with open(table, mode = 'w') as f:\n",
    "    out.to_csv(f,index=False, header=f.tell()==0) #append tableOut with FILE-OSN and IMG-PATH values\n",
    "    print(\"\\n\")\n",
    "    print(\"lookup table updated with HandPrint output locations for \" + str(types) + \"-type transcriptions\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "#input/output\n",
    "paths = '....txt'#plain text list of urls or filenames (pre-existing)\n",
    "target = '...' #HP outputs (pre-existing)\n",
    "textOut = '....txt'#Bag-of-Words output\n",
    "\n",
    "#declarations\n",
    "BoW = open(textOut, \"a\")\n",
    "\n",
    "#append bag-of-words with headers and transcriptions\n",
    "for path in sorted(Path(target).rglob('*.txt')):\n",
    "    header = path.stem.split('.')\n",
    "    BoW.write(\"\\n\")\n",
    "    BoW.write(header[0]) #write headers location to bag-of-words\n",
    "    BoW.write(\"\\n\")\n",
    "    print((header[0]) + \" added to bag-of-words\")\n",
    "    absolute = (str(path.parent) + \"/\" + path.name)\n",
    "    contents = open(absolute, \"r\") \n",
    "    BoW.write(str(contents.read())) #write transcriptions to bag-of-words\n",
    "    print((\"Transcription for \" + header[0]) + \" added to bag-of-words\")\n",
    "    BoW.write(\"\\n\")\n",
    "BoW.close()\n",
    "print(\"Bag-of-words generated and output from corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes corpus Bag-of-Words and returns json dump of common nouns, Sketchfab models/uids, and relative percentage of occurance\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import json\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "#declarations\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "BoW = \"....txt\"\n",
    "SKETCHFAB_API_URL = \"https://api.sketchfab.com/v3/search?type=models&count=1\" #note count parameter\n",
    "API_TOKEN = '046f786a59eb4112b9b89ba26a85f85f'\n",
    "output = \".../objects.txt\"\n",
    "count = 0\n",
    "start = time.time()\n",
    "\n",
    "#data structures\n",
    "nouns = [] # all nounds in Bag-of-Words\n",
    "freqs = [] # noun appearances in BoW\n",
    "objects = {} # key = [common nouns]; value(s) = [uids, percentage, model name]\n",
    "    \n",
    "#parse Bag-of-Words parts-of-speech with Spacy (english)\n",
    "with open(BoW, encoding=\"utf-8\") as file:\n",
    "    iliad = file.read()\n",
    "document = nlp(iliad)\n",
    "document.text\n",
    "for token in document:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token.text)\n",
    "file.close()\n",
    "\n",
    "#frequency & dictionary with common words and percentages\n",
    "word_freq = Counter(nouns)\n",
    "common = word_freq.most_common(100)\n",
    "for word in common:\n",
    "    freqs.append(word[1])\n",
    "Sum = sum(freqs)\n",
    "for word in common:\n",
    "    flowt = (word[1] / Sum) * 100\n",
    "    percentage = round(flowt, 2)\n",
    "    objects[word[0]] = [percentage]\n",
    "\n",
    "#Sketchfab API payload function \n",
    "##From https://sketchfab.com/developers/data-api/v3/python#example-python-model\n",
    "def _get_request_payload(*, data=None, files=None, json_payload=False):\n",
    "    \"\"\"Helper method that returns the authentication token and proper content type depending on\n",
    "    whether or not we use JSON payload.\"\"\"\n",
    "    data = data or {}\n",
    "    files = files or {}\n",
    "    headers = {'Authorization': 'Token {}'.format(API_TOKEN)}\n",
    "    if json_payload:\n",
    "        headers.update({'Content-Type': 'application/json'})\n",
    "        data = json.dumps(data)\n",
    "    return {'data': data, 'files': files, 'headers': headers}\n",
    "\n",
    "#query sketchfabs with nouns in BoW and return/write list of uids + model names\n",
    "for key in objects.keys():\n",
    "    print(\"Searching: \" + str(key))\n",
    "    print(\"\\n\")\n",
    "    query = (\"&q=\"+(str(key))+\"&downloadable=true\")\n",
    "    search_endpoint = f'{SKETCHFAB_API_URL + query}'\n",
    "    payload = _get_request_payload() \n",
    "    response = requests.get(search_endpoint, **payload)\n",
    "    data = response.json()\n",
    "    for item in range(len(data['results'])):\n",
    "        uid = (data['results'][item]['uid'])\n",
    "        name = (str((data['results'][item]['name'])))\n",
    "        objects[key] += [name, uid]\n",
    "        print(\"the following model been located: \")\n",
    "        print(str((data['results'][item]['name']))+(\" \\nuid: \")+(data['results'][item]['uid']))\n",
    "        print(\"\\n\")\n",
    "        uids.append(name +\", \"+ uid)\n",
    "\n",
    "#write to disk and close program\n",
    "with open(output, 'w') as file:\n",
    "    file.write(json.dumps(objects)) \n",
    "file.close()\n",
    "print(\"Objects written to disk\")\n",
    "print(\"\\n\")\n",
    "end = time.time()\n",
    "print(str(end - start) + \" seconds elapsed\" )\n",
    "print(\"\\n\")\n",
    "print(\"have a nice day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Launches Blender from terminal and initiates bpy script\n",
    "!/.../Blender --python /.../operator_file_import.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef183be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO DO:\n",
    "#Place search results in the scene, without overlap, using frequencies\n",
    "#export scene (gltf) for use in Hubs\n",
    "#automate scene change function in Hubs, based on newly created .glb\n",
    "#Compare: \n",
    "##viewing angle limitations and text legibility)\n",
    "##                     vs.\n",
    "##non-accidental (3D) object properties and object recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
