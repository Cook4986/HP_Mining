{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6cf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Longhand - M. Cook - 2022\n",
    "##Takes text corpus and returns immersive visualization\n",
    "##https://github.com/Cook4986/Longhand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes Bag-of-Words and returns json dump of common nouns, Sketchfab models/uids, and relative percentage of occurance\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import json\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "#select language model (https://spacy.io/models)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 100000000\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "#I/O\n",
    "BoW = \"...\"#plaintext bag-of-words input\n",
    "output = \"...\" #See: \"Objects\" declaration, below, for structure\n",
    "\n",
    "#declarations\n",
    "model_size = 100000 #face count\n",
    "SKETCHFAB_API_URL = \"https://api.sketchfab.com/v3/search?type=models&count=24&max_face_count=\" + str(model_size) #note count parameter\n",
    "API_TOKEN = '...' #keep private\n",
    "results = 50 #target number of models, potentially limited by NER outputs\n",
    "slug = [\"categories=food-drink\"]#limits search\n",
    "start = time.time()\n",
    "\n",
    "#data structures\n",
    "nouns = [] #nouns in Bag-of-Words\n",
    "entities = [] #named entities\n",
    "freqs = [] # noun appearance frequencies\n",
    "objects = {} # key = common nouns; value(s) = [relative percentage of total objects, UID, model name, URL]\n",
    "\n",
    "#parse Bag-of-Words parts-of-speech with SpaCy\n",
    "with open(BoW, encoding=\"utf-8\") as file:\n",
    "    print(\"Tokenizing text...\")\n",
    "    print(\"\\n\")\n",
    "    iliad = file.read()\n",
    "document = nlp(iliad)\n",
    "\n",
    "#collate nouns or named entities (\"results\") in corpus, by frequency\n",
    "for token in document:\n",
    "    if (token not in stopwords) & (token.pos_ == 'NOUN'): \n",
    "            nouns.append(token.text)         \n",
    "labelsEN = [\"PRODUCT\"]\n",
    "#[\"PERSON\",PRODUCT\",\"EVENT\",\"FAC\",\"WORK_OF_ART\",\"LOC\",\"NORP\",\"GPE\",\"ORG\"] \n",
    "for entity in document.ents:\n",
    "    for label in labelsEN:\n",
    "        if (token not in stopwords) & (entity.label_ == label):\n",
    "            entities.append(entity.text)\n",
    "word_freq = Counter(nouns) #update for nouns or entities\n",
    "common = word_freq.most_common(results)\n",
    "\n",
    "#Sketchfab API payload function \n",
    "##From https://sketchfab.com/developers/data-api/v3/python#example-python-model\n",
    "def _get_request_payload(*, data=None, files=None, json_payload=False):\n",
    "    \"\"\"Helper method that returns the authentication token and proper content type depending on\n",
    "    whether or not we use JSON payload.\"\"\"\n",
    "    data = data or {}\n",
    "    files = files or {}\n",
    "    headers = {'Authorization': 'Token {}'.format(API_TOKEN)}\n",
    "    if json_payload:\n",
    "        headers.update({'Content-Type': 'application/json'})\n",
    "        data = json.dumps(data)\n",
    "    return {'data': data, 'files': files, 'headers': headers}\n",
    "\n",
    "#query sketchfabs with tokens and compile object dictionary with results\n",
    "for word in common:\n",
    "    key = str(word[0])\n",
    "    print(\"Searching: \" + key)\n",
    "    print(\"\\n\")\n",
    "    query = (\"&q=\"+(key)+\"&\"+ slug[0] + \"&downloadable=true\") #granular search, see: \"slug\" declaration, above\n",
    "    #query = (\"&q=\"+(key)+\"&downloadable=true\")\n",
    "    search_endpoint = f'{SKETCHFAB_API_URL + query}'\n",
    "    payload = _get_request_payload() \n",
    "    response = requests.get(search_endpoint, **payload)\n",
    "    data = response.json()\n",
    "    for item in range(len(data['results'])):\n",
    "        url = (data['results'][item]['uri'])\n",
    "        uid = (data['results'][item]['uid'])\n",
    "        name = (str((data['results'][item]['name'])))\n",
    "        size = int(data['results'][item]['faceCount'])\n",
    "        if (key.lower() in name.lower()) & (key not in objects):\n",
    "            freqs.append(word[1])\n",
    "            objects[word[0]] = [word[1]]\n",
    "            objects[key] += [name, uid, url, size]\n",
    "print(\"\\n\")      \n",
    "\n",
    "#write object dictionary to disk \n",
    "with open(output, 'w') as file:\n",
    "    file.write(json.dumps(objects)) \n",
    "file.close()\n",
    "\n",
    "#print hits and relative percentages in corpus\n",
    "Sum = sum(freqs)\n",
    "for key,value in objects.items():\n",
    "    print(\"Model located for '\" + key + \"':\")\n",
    "    print(value[1].center(24))\n",
    "    flowt = (value[0] / Sum) * 100\n",
    "    percentage = round(flowt, 2)\n",
    "    print(\"Represents \" + str(percentage) + \"% of models identified.\")\n",
    "    if value[4] > 10000:\n",
    "                print(\"Warning: Model size exceeds 10000 faces \" + \"(\" + (str(size)) + \" faces)\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(str(len(objects)) + \" most suitable models (of \"+ str(results) + \") located on Sketchfab written to disk\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#terminate program\n",
    "end = time.time()\n",
    "print(str(end - start) + \" seconds elapsed\" )\n",
    "print(\"\\n\")\n",
    "print(\"have a nice day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b93c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Launches Blender from terminal and initiates model download script\n",
    "!/Applications/Blender.app/Contents/MacOS/Blender --python .../Longhand_downloader.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes/To-Do:\n",
    "\n",
    "##Add \"working directory\", across scripts, to automate declarations\n",
    "##lemmatize tokens?\n",
    "##Mesh collision detection\n",
    "##user-selected working directory + \"name your output\" user input (for objects file)\n",
    "#u#ser-selected \"nouns or entities\"\n",
    "##parse entire directory of files (glob)\n",
    "##GUI: \"Select Bag-of-Words\", Select working directory\", \"How many objects?\", Nouns or Entities\", \"Entity Categories\"\n",
    "##Print BoW file size\n",
    "##spell-check?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
